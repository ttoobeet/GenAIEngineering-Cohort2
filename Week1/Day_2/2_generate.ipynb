{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c46f2e",
   "metadata": {},
   "source": [
    "# Text Generation: Understanding the Role of Probabilities\n",
    "\n",
    "Text generation is a fascinating application of machine learning that powers everything from chatbots to creative writing assistants. At its core, modern text generation relies on probabilistic modeling—a mathematical approach that treats language as a series of statistical predictions.\n",
    "\n",
    "## The Foundation: Probability Distributions\n",
    "\n",
    "When a language model generates text, it's essentially answering the question: \"Given the text so far, what word is likely to come next?\" The model maintains a probability distribution across its entire vocabulary, assigning higher probabilities to words that make more sense in the current context.\n",
    "\n",
    "For example, if the partial sentence is \"The chef cooked a delicious,\" the model might assign:\n",
    "- \"meal\" → 22% probability\n",
    "- \"dish\" → 18% probability \n",
    "- \"steak\" → 12% probability\n",
    "- thousands of other words → smaller probabilities\n",
    "\n",
    "## The Generation Process\n",
    "\n",
    "Text generation typically works through these steps:\n",
    "\n",
    "1. **Conditioning**: The model processes the input text (the \"prompt\" or context)\n",
    "2. **Prediction**: It calculates probability scores for each possible next token\n",
    "3. **Sampling**: It selects the next token based on these probabilities\n",
    "4. **Iteration**: The process repeats with the newly expanded text\n",
    "\n",
    "## Sampling Methods\n",
    "\n",
    "How a model selects the next word significantly affects the output:\n",
    "\n",
    "- **Greedy sampling**: Always choose the highest probability token\n",
    "- **Temperature sampling**: Adjust how \"deterministic\" vs \"creative\" the choices are\n",
    "- **Top-k sampling**: Only consider the k most likely tokens\n",
    "- **Top-p (nucleus) sampling**: Only consider tokens whose cumulative probability exceeds threshold p\n",
    "\n",
    "## Temperature: Controlling Randomness\n",
    "\n",
    "Temperature is a hyperparameter that controls how \"risky\" the model's choices are:\n",
    "\n",
    "- **Low temperature** (0.1-0.5): More predictable, focused, repetitive outputs\n",
    "- **Medium temperature** (0.6-0.9): Balanced between coherence and creativity\n",
    "- **High temperature** (1.0+): More surprising, diverse, and sometimes chaotic outputs\n",
    "\n",
    "By understanding and adjusting these probabilistic mechanisms, developers can fine-tune text generation systems to produce content that strikes the right balance between predictability and creativity for their specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7920c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import PyPDF2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071ef53",
   "metadata": {},
   "source": [
    "### This function extracts text from a PDF file and performs basic text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2cdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daee0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf('pdfs/IndianBudget2025.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb289e",
   "metadata": {},
   "source": [
    "# Text Tokenization: Breaking Text into Meaningful Units\n",
    "\n",
    "When working with natural language processing (NLP), one of the fundamental preprocessing steps is tokenization. Tokenization is the process of converting raw text into smaller, more manageable units called tokens. These tokens form the basis for further text analysis.\n",
    "\n",
    "## Why Tokenize Text?\n",
    "\n",
    "Tokenization serves several key purposes:\n",
    "\n",
    "1. **Text Standardization** - Breaking text into consistent units\n",
    "2. **Feature Extraction** - Preparing text for statistical analysis\n",
    "3. **Vocabulary Building** - Creating a dictionary of terms for analysis\n",
    "4. **Enabling Count-Based Methods** - Supporting techniques like TF-IDF or bag-of-words\n",
    "\n",
    "## Simple Word Tokenization\n",
    "\n",
    "The most common form of tokenization splits text into individual words. While there are sophisticated tokenization techniques available through libraries like NLTK, spaCy, or transformers, sometimes a simple approach using regular expressions is sufficient for basic tasks.\n",
    "\n",
    "Here's a straightforward function that tokenizes text into words:\n",
    "\n",
    "\n",
    "With this function, you can easily convert sentences or paragraphs into a list of words that can be counted, analyzed, or used as input for more advanced NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfea91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text into words.\"\"\"\n",
    "    # Simple tokenization: split by spaces and remove punctuation\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f3f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb75f2",
   "metadata": {},
   "source": [
    "# Building a Simple Language Model with Unigram Probabilities\n",
    "\n",
    "After tokenizing our text, the next step in creating a basic language model is to analyze word sequences and their probabilities. One of the simplest approaches is to build a unigram model, which calculates the probability of each word following another word in a corpus.\n",
    "\n",
    "## What is a Unigram Model?\n",
    "\n",
    "A unigram model (more accurately called a bigram model in this context) captures the probability of one word following another. By analyzing these transition probabilities, we can:\n",
    "\n",
    "1. Predict likely next words in a sequence\n",
    "2. Generate text that follows similar patterns to the training data\n",
    "3. Evaluate the likelihood of specific word sequences\n",
    "\n",
    "## Counting Word Transitions\n",
    "\n",
    "The first step is to count every instance where one word follows another in our corpus:\n",
    "\n",
    "This code creates a statistical model of word transitions by:\n",
    "- Counting how often each word appears after every other word\n",
    "- Converting these counts into probabilities by dividing by the total occurrences\n",
    "- Storing these probabilities in a structured format\n",
    "- Saving the model to a JSON file for later use\n",
    "\n",
    "With this probability distribution, we can now predict likely next words or generate text that follows similar patterns to our original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bfde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = defaultdict(Counter)\n",
    "unigram_probs = {}\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "      current_word = tokens[i]\n",
    "      next_word = tokens[i + 1]\n",
    "      unigram_counts[current_word][next_word] += 1\n",
    "\n",
    "for sequence, next_words in unigram_counts.items():\n",
    "    total_occurrences = sum(next_words.values())\n",
    "    sequence_probs = {word: count / total_occurrences for word, count in next_words.items()}\n",
    "    unigram_probs[sequence] = sequence_probs\n",
    "with open('unigram_probs.json', 'w') as f:\n",
    "    json.dump(unigram_probs, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766b4df",
   "metadata": {},
   "source": [
    "# Extending to Bigram and Trigram Models: Capturing Longer Context\n",
    "\n",
    "Building on our previous unigram (bigram) model, we can enhance our language model by incorporating longer contexts. A trigram model looks at sequences of three consecutive words to predict the fourth word, providing more contextual awareness than simpler models.\n",
    "\n",
    "## From Unigrams to Trigrams\n",
    "\n",
    "While our previous model captured the relationship between pairs of words, trigram models capture the relationship between two and three words and the word that follows them. This provides several advantages:\n",
    "\n",
    "1. More natural and coherent text generation\n",
    "2. Better prediction accuracy with longer context\n",
    "3. Ability to capture more complex language patterns\n",
    "\n",
    "## Implementing a Bigram and Trigram Model\n",
    "\n",
    "Similar to our unigram approach, we'll count sequences and calculate probabilities:\n",
    "\n",
    "This code follows the same pattern as our unigram model but uses two-word (bigrams) and three-word sequences (trigrams) as the context. The resulting probability distribution captures how likely each word is to follow a specific three-word sequence in our corpus.\n",
    "\n",
    "By comparing this with our previous unigram model, we can see how increasing the context length affects the model's ability to capture language patterns. This approach forms the foundation of n-gram language models, where n can be any number representing the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e38e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = defaultdict(Counter)\n",
    "bigram_probs = {}\n",
    "\n",
    "for i in range(len(tokens) - 2):\n",
    "    current_bigram = tokens[i]+' '+tokens[i + 1]\n",
    "    next_word = tokens[i + 2]\n",
    "    bigram_counts[current_bigram][next_word] += 1\n",
    "    \n",
    "for sequence, next_words in bigram_counts.items():\n",
    "    total_occurrences = sum(next_words.values())\n",
    "    sequence_probs = {word: count / total_occurrences for word, count in next_words.items()}\n",
    "    bigram_probs[sequence] = sequence_probs\n",
    "    \n",
    "with open('bigram_probs.json', 'w') as f:\n",
    "    json.dump(bigram_probs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbe253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts = defaultdict(Counter)\n",
    "trigram_probs = {}\n",
    "\n",
    "for i in range(len(tokens) - 3):\n",
    "    current_trigram = tokens[i]+' '+tokens[i + 1]+' '+tokens[i + 2]\n",
    "    next_word = tokens[i + 3]\n",
    "    trigram_counts[current_trigram][next_word] += 1\n",
    "\n",
    "for sequence, next_words in trigram_counts.items():\n",
    "    total_occurrences = sum(next_words.values())\n",
    "    sequence_probs = {word: count / total_occurrences for word, count in next_words.items()}\n",
    "    trigram_probs[sequence] = sequence_probs\n",
    "with open('trigram_probs.json', 'w') as f:\n",
    "    json.dump(trigram_probs, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aadce6",
   "metadata": {},
   "source": [
    "# Generating Text with N-gram Models\n",
    "\n",
    "After building our probability models (unigram, bigram and trigram), we can use them to generate new text. The process involves selecting a starting point and then using our probability distributions to select each subsequent word.\n",
    "\n",
    "\n",
    "## Initializing the Generation Process\n",
    "\n",
    "The first step in text generation is selecting a starting point. We can either choose a specific starting word or randomly select one from our model's vocabulary:\n",
    "\n",
    "\n",
    "This code:\n",
    "- Sets up the random number generator (either with a fixed seed for reproducibility or randomly)\n",
    "- Selects a random starting word from the words we've seen in our training data\n",
    "- Prints this starting word, which will be the beginning of our generated text\n",
    "\n",
    "From here, we can continue the generation process by repeatedly sampling from our probability distributions to create a chain of words that follows the statistical patterns of our original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4420e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minister\n"
     ]
    }
   ],
   "source": [
    "seed = None\n",
    "if seed:\n",
    "  random.seed(seed)\n",
    "else:\n",
    "  random.seed()\n",
    "  \n",
    "current = random.choice(list(unigram_probs.keys()))\n",
    "print(current)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90c7c8",
   "metadata": {},
   "source": [
    "# Backoff Model for Text Generation\n",
    "\n",
    "After selecting our starting word, we need to continue the generation process by predicting subsequent words. For more robust text generation, we can implement a backoff model that prioritizes longer contexts when available but \"backs off\" to shorter contexts when necessary.\n",
    "\n",
    "## Implementing a Backoff Strategy\n",
    "\n",
    "The backoff approach tries to use the most specific model first (trigram), then falls back to less specific models (bigram, then unigram) when needed:\n",
    "\n",
    "This code:\n",
    "\n",
    "1. Sets a target length for our generated text (100 words)\n",
    "2. Initializes our text with the starting word\n",
    "3. For each subsequent word:\n",
    "   - First tries to use the trigram model (3-word context)\n",
    "   - If that fails, backs off to the bigram model (2-word context)\n",
    "   - If that fails too, uses the unigram model (1-word context)\n",
    "4. Samples the next word according to the appropriate probability distribution\n",
    "5. Adds the sampled word to our growing text\n",
    "6. Finally prints the complete generated text\n",
    "\n",
    "This backoff strategy makes our text generation more robust, seamlessly handling cases where specific longer contexts haven't been seen in our training data. The result is generated text that better captures the statistical patterns of natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155e042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10']\n",
      "to 20 and reduce the bcd to 5 on fish hydrolysate for manufacture of surimi analogue products for export 30 5 36 s no commodity from per cent to per cent 1 crust leather hides and skins 20 0 f trade facilitation measures f 1 increase in duration for export of foreign origin goods that were imported for repairs from 6 months to 1 year further extendable by one year i now pro pose to revise the bcd rate on knitted fabrics covered under tariff heading 8702 40 4 sws 20 20 aidc 40 5 candles tapers and the "
     ]
    }
   ],
   "source": [
    "num_words=100\n",
    "generated_text = [current]\n",
    "print(generated_text)\n",
    "for _ in range(num_words - 1):\n",
    "    # print(words)\n",
    "    if len(generated_text)>=3 and ' '.join(generated_text[-3:]) in trigram_probs:\n",
    "        words_list, probs = zip(*trigram_probs[' '.join(generated_text[-3:])].items())\n",
    "    elif len(generated_text)>=2 and ' '.join(generated_text[-2:]) in bigram_probs:\n",
    "        words_list, probs = zip(*bigram_probs[' '.join(generated_text[-2:])].items())\n",
    "    elif generated_text[-1] in unigram_probs:\n",
    "        words_list, probs = zip(*unigram_probs[current].items())\n",
    "    current = random.choices(words_list, weights=probs, k=1)[0]\n",
    "    generated_text.append(current)\n",
    "    print(current, end=' ', flush=True)\n",
    "    time.sleep(0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
