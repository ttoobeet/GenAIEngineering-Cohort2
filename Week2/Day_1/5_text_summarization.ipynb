{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536ac465",
   "metadata": {},
   "source": [
    "Google Colab: https://colab.research.google.com/drive/1-JKo2uBVK4fxCYY6uDznzBdLv9Z0dm9Q?usp=sharing \n",
    "\n",
    "HuggingFace: https://huggingface.co/facebook/bart-large-cnn \n",
    "\n",
    "## Libraries Explained\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file into the application's environment, helping manage configuration separately from code.\n",
    "\n",
    "- **huggingface_hub**: \n",
    "  - **HfApi**: Provides programmatic access to the Hugging Face model hub for uploading, downloading, and managing models.\n",
    "  - **hf_hub_download**: Simplifies downloading model files from the Hugging Face hub to your local environment.\n",
    "\n",
    "- **transformers**: Offers pre-trained models for natural language processing tasks. The `pipeline` function specifically provides an easy-to-use interface for common NLP tasks like text generation, sentiment analysis, and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7002eb",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff45713",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Model Reference\n",
    "\n",
    "[facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)\n",
    "\n",
    "\n",
    "\n",
    "# Facebook BART Large CNN Model\n",
    "\n",
    "## Model Overview\n",
    "This reference points to Facebook AI's BART large model that has been fine-tuned specifically for summarization tasks using the CNN/Daily Mail dataset.\n",
    "\n",
    "## Key Specifications\n",
    "- **Architecture**: BART (Bidirectional and Auto-Regressive Transformers)\n",
    "- **Size**: Large variant (400M parameters)\n",
    "- **Fine-tuning**: CNN/Daily Mail news article summarization dataset\n",
    "- **Developer**: Facebook AI Research (now Meta AI)\n",
    "- **Primary Task**: Text summarization\n",
    "\n",
    "## Model Capabilities\n",
    "- Generates concise and coherent summaries of longer texts\n",
    "- Particularly effective for news article summarization\n",
    "- Maintains key information while reducing text length\n",
    "- Produces abstractive summaries (not just extractive)\n",
    "- Can handle documents of moderate length (typically up to ~1024 tokens)\n",
    "\n",
    "\n",
    "This model serves as an excellent option for applications requiring high-quality text summarization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c361954",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='facebook/bart-large-cnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4673a",
   "metadata": {},
   "source": [
    "\n",
    "# Downloading Specific Model Files from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code snippet demonstrates how to selectively download specific files from a Hugging Face model repository:\n",
    "\n",
    "1. **File Definition**: First, a list of commonly required files for transformer models is defined, with comments explaining each file's purpose:\n",
    "   - Vocabulary files for tokenization\n",
    "   - Configuration files for model architecture\n",
    "   - Tokenizer files for text preprocessing\n",
    "   - Model weights in different formats (PyTorch and SafeTensors)\n",
    "\n",
    "2. **Selective Download**: The code iterates through each file in the list and:\n",
    "   - Attempts to download it using `hf_hub_download()`\n",
    "   - Specifies the model repository via `repo_id=hf_reference`\n",
    "   - Saves files to a local directory structure based on the model name\n",
    "   - Prints the local path where each file is saved\n",
    "\n",
    "3. **Error Handling**: The try-except block catches and reports any download failures, allowing the process to continue even if certain files aren't available for the specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of required files\n",
    "# required_files = [\n",
    "#     \"vocab.txt\",          # Vocabulary file (if applicable)\n",
    "#     \"vocab.json\",          # Vocabulary file (if applicable)       \n",
    "#     \"config.json\",        # Model configuration\n",
    "#     \"tokenizer.json\",     # Tokenizer configuration (if applicable)\n",
    "#     \"merges.txt\",         # BPE merge rules file (if applicable)\n",
    "#     \"pytorch_model.bin\",  # Model weights\n",
    "#     \"model.safetensors\",  # Alternative model weights format\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Download only the required files\n",
    "# for file_name in required_files:\n",
    "#     try:\n",
    "#         print()\n",
    "#         print(f\"Attempting to download: {file_name}\")\n",
    "#         local_path = hf_hub_download(repo_id=hf_reference, filename=file_name, local_dir=f\"models/{hf_reference.split('/')[1]}\")\n",
    "#         print(f\"Saved to: {local_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not download {file_name}: {e}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f50228",
   "metadata": {},
   "source": [
    "# Setting Up BART for Text Summarization\n",
    "\n",
    "This code initializes a text summarization pipeline using Facebook's BART model fine-tuned on the CNN/Daily Mail dataset.\n",
    "\n",
    "## Pipeline Configuration\n",
    "\n",
    "- **Task**: `\"summarization\"` - Creates concise summaries of longer texts\n",
    "- **Model**: Uses the model specified in `hf_reference` (facebook/bart-large-cnn)\n",
    "- **Storage**: The model is cached in `hf_model_cache` for efficient reuse\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- The `pipeline()` function from Hugging Face's Transformers library provides a streamlined API\n",
    "- Automatically handles tokenization, model inference, and text generation\n",
    "- Downloads and caches the model weights on first use\n",
    "- Makes summarization accessible with minimal code\n",
    "\n",
    "## Alternative Implementation (Commented)\n",
    "\n",
    "The commented line shows an alternative approach:\n",
    "- Would use a locally downloaded version of the model\n",
    "- Extracts the model name from the reference using string splitting\n",
    "- Assumes the model exists in a local `models/` directory\n",
    "- Useful for offline use or environments with limited connectivity\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "```python\n",
    "# Article to summarize\n",
    "long_text = \"\"\"\n",
    "Researchers have discovered a new species of deep-sea fish living at depths of over \n",
    "7,000 meters in the Mariana Trench. The discovery was made during an expedition that \n",
    "used specialized submersibles to explore the ocean's deepest regions. The fish exhibits \n",
    "unique adaptations to the extreme pressure, including a gelatinous body structure and \n",
    "specialized cellular components. Scientists believe studying these adaptations could \n",
    "provide insights into pressure-resistant biomaterials. The findings were published in \n",
    "the journal Nature and represent the deepest living vertebrate species documented to date.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary\n",
    "summary = hf_model_cache(long_text, max_length=100, min_length=30, do_sample=False)\n",
    "\n",
    "# Print the summarized text\n",
    "print(summary[0]['summary_text'])\n",
    "```\n",
    "\n",
    "This pipeline enables efficient text summarization for applications such as news digests, document analysis, and content curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_cache = pipeline(\"summarization\", model=hf_reference)\n",
    "# hf_model_local = pipeline(\"summarization\", model=f\"models/{hf_reference.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fce28f",
   "metadata": {},
   "source": [
    "# Summarizing Cricket News with BART\n",
    "\n",
    "This code uses the BART-large-CNN model to generate a concise summary of a cricket news article about Team India and captain Rohit Sharma.\n",
    "\n",
    "## Input Article Content\n",
    "\n",
    "The text describes:\n",
    "- Team India's poor performance in the Border-Gavaskar Trophy against Australia\n",
    "- Potential leadership changes with Rohit Sharma's captaincy under scrutiny\n",
    "- Reports of a senior player positioning himself as an interim captain\n",
    "- Rohit's struggling batting form and potential career decisions\n",
    "\n",
    "## Summarization Configuration\n",
    "\n",
    "- **Model**: facebook/bart-large-cnn (via the hf_model_cache pipeline)\n",
    "- **Parameters**:\n",
    "  - `do_sample=False`: Uses greedy decoding for deterministic output\n",
    "  - Default max_length (typically 142 tokens or ~100 words)\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- The commented line shows an alternative approach to dynamically set the summary length\n",
    "  - Would limit the summary to approximately 1/4 of the word count of the original text\n",
    "  - Uses `len(txt.split(\" \"))` to count words, though variable `txt` is undefined (should be `text`)\n",
    "\n",
    "- The summarized result is stored in the `result` variable but not printed in this code snippet\n",
    "\n",
    "This demonstrates how the BART model can condense news articles while preserving the key information about team performance and leadership issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Team India's below-par performance in the Border-Gavaskar Trophy could see big changes in the team and the leadership group. Rohit Sharma's captaincy is under the scanner and the selectors could take a call on him if India fail to reach the World Test Championship final. He has also struggled with the bat and only managed 31 runs in the ongoing series.\n",
    "Amid India's poor performance in Australia, the Indian Express has reported that a senior player is portraying to be 'Mr Fix-it.\" The report states that the senior player is ready to project himself as an interim option for captaincy as he isn't convinced about the young players. The report doesn't mention the name of the senior player.\n",
    "The report adds that Rohit may take a call about his career after the Border-Gavaskar Trophy. He made his ODI and T20I captaincy debut in 2007. Rohit made his Test debut in 2013.\n",
    "'''\n",
    "\n",
    "# Summarize the text\n",
    "# print(hf_model_cache(text, max_length=100, do_sample=False))\n",
    "result=hf_model_cache(text,  do_sample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"Rohit Sharma's captaincy is under the scanner and the selectors could take a call on him if India fail to reach the World Test Championship final. He has also struggled with the bat and only managed 31 runs in the ongoing series. The report doesn't mention the name of the senior player.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb652c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text), len(result[0]['summary_text']))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827796a5",
   "metadata": {},
   "source": [
    "\n",
    "# Serialize and Save Model Information from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code demonstrates how to retrieve, serialize, and save detailed model information from the Hugging Face Hub:\n",
    "\n",
    "1. **Serialization Function**: The `serialize_object()` function handles complex objects recursively:\n",
    "   - Converts datetime objects to ISO format strings\n",
    "   - Transforms objects with `__dict__` attributes into dictionaries\n",
    "   - Processes nested lists and dictionaries\n",
    "   - Preserves primitive data types\n",
    "\n",
    "2. **API Interaction**: Creates an instance of the Hugging Face API client\n",
    "\n",
    "3. **Model Information**: Fetches comprehensive metadata about the specified model using `api.model_info()`\n",
    "\n",
    "4. **File Operations**: \n",
    "   - Extracts the model name from the reference path\n",
    "   - Creates a JSON file named after the model\n",
    "   - Serializes the model information and writes it to the file\n",
    "\n",
    "This allows for local storage of model metadata for later reference or analysis, particularly useful for model governance, versioning, and documentation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "api = HfApi()\n",
    "with open(f\"models/{hf_reference.split('/')[1]}.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(serialize_object(api.model_info(hf_reference))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
