{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb5b57a",
   "metadata": {},
   "source": [
    "Google Colab: https://colab.research.google.com/drive/1Frl-jmNl99n1mpLAWzQQHmkseOkxR0UB?authuser=1#scrollTo=536ac465\n",
    "HuggingFace: https://huggingface.co/openai-community/gpt2-large\n",
    "\n",
    "## Benefits\n",
    "- Free GPU/TPU access\n",
    "- Pre-installed ML libraries\n",
    "- Google Drive integration\n",
    "- Real-time collaboration\n",
    "\n",
    "## Quick Start\n",
    "1. Open the link\n",
    "2. Make a copy (File → Save in Drive)\n",
    "3. Enable GPU (Runtime → Change runtime type)\n",
    "4. Run cells (Shift+Enter)\n",
    "\n",
    "## Tips\n",
    "- Monitor resources (Runtime → Manage sessions)\n",
    "- Save regularly (auto-disconnects after inactivity)\n",
    "- Mount Drive for persistent storage\n",
    "- Load large datasets progressively\n",
    "\n",
    "## Limitations\n",
    "- ~12 hour sessions (free tier)\n",
    "- Variable GPU availability\n",
    "- Requires stable internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ac465",
   "metadata": {},
   "source": [
    "\n",
    "## Libraries Explained\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file into the application's environment, helping manage configuration separately from code.\n",
    "\n",
    "- **huggingface_hub**: \n",
    "  - **HfApi**: Provides programmatic access to the Hugging Face model hub for uploading, downloading, and managing models.\n",
    "  - **hf_hub_download**: Simplifies downloading model files from the Hugging Face hub to your local environment.\n",
    "\n",
    "- **transformers**: Offers pre-trained models for natural language processing tasks. The `pipeline` function specifically provides an easy-to-use interface for common NLP tasks like text generation, sentiment analysis, and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7002eb",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff45713",
   "metadata": {},
   "source": [
    "\n",
    "# OpenAI GPT-2 Large Model\n",
    "\n",
    "[openai-community/gpt2-large](https://huggingface.co/openai-community/gpt2-large)\n",
    "\n",
    "\n",
    "\n",
    "## Model Overview\n",
    "This reference points to the GPT-2 Large model hosted by the OpenAI community on Hugging Face's model hub.\n",
    "\n",
    "## Key Specifications\n",
    "- **Architecture**: GPT-2 (Generative Pre-trained Transformer 2)\n",
    "- **Size**: Large variant (774M parameters)\n",
    "- **Training**: Unsupervised language modeling on 40GB of internet text\n",
    "- **Developer**: Originally developed by OpenAI, now maintained by the community\n",
    "- **Capabilities**: Text generation, completion, and various NLP tasks\n",
    "\n",
    "## Model Details\n",
    "- **Context Window**: 1024 tokens\n",
    "- **Vocabulary Size**: 50,257 tokens\n",
    "- **Output**: Autoregressive text generation\n",
    "- **Language**: English\n",
    "\n",
    "\n",
    "While smaller than state-of-the-art models like GPT-4, GPT-2 Large remains a powerful language model for text generation, summarization, and other creative text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c361954",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='openai-community/gpt2-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4673a",
   "metadata": {},
   "source": [
    "\n",
    "# Downloading Specific Model Files from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code snippet demonstrates how to selectively download specific files from a Hugging Face model repository:\n",
    "\n",
    "1. **File Definition**: First, a list of commonly required files for transformer models is defined, with comments explaining each file's purpose:\n",
    "   - Vocabulary files for tokenization\n",
    "   - Configuration files for model architecture\n",
    "   - Tokenizer files for text preprocessing\n",
    "   - Model weights in different formats (PyTorch and SafeTensors)\n",
    "\n",
    "2. **Selective Download**: The code iterates through each file in the list and:\n",
    "   - Attempts to download it using `hf_hub_download()`\n",
    "   - Specifies the model repository via `repo_id=hf_reference`\n",
    "   - Saves files to a local directory structure based on the model name\n",
    "   - Prints the local path where each file is saved\n",
    "\n",
    "3. **Error Handling**: The try-except block catches and reports any download failures, allowing the process to continue even if certain files aren't available for the specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78b4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of required files\n",
    "# required_files = [\n",
    "#     \"vocab.txt\",          # Vocabulary file (if applicable)\n",
    "#     \"vocab.json\",          # Vocabulary file (if applicable)       \n",
    "#     \"config.json\",        # Model configuration\n",
    "#     \"tokenizer.json\",     # Tokenizer configuration (if applicable)\n",
    "#     \"merges.txt\",         # BPE merge rules file (if applicable)\n",
    "#     \"pytorch_model.bin\",  # Model weights\n",
    "#     \"model.safetensors\",  # Alternative model weights format\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Download only the required files\n",
    "# for file_name in required_files:\n",
    "#     try:\n",
    "#         print()\n",
    "#         print(f\"Attempting to download: {file_name}\")\n",
    "#         local_path = hf_hub_download(repo_id=hf_reference, filename=file_name, local_dir=f\"models/{hf_reference.split('/')[1]}\")\n",
    "#         print(f\"Saved to: {local_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not download {file_name}: {e}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f50228",
   "metadata": {},
   "source": [
    "\n",
    "# Setting Up GPT-2 Large for Text Generation\n",
    "\n",
    "This code initializes a text generation pipeline using the GPT-2 Large model from Hugging Face's model hub.\n",
    "\n",
    "## Pipeline Configuration\n",
    "\n",
    "- **Task**: `\"text-generation\"` - Creates natural language text completions\n",
    "- **Model Source**: Uses the model specified in `hf_reference` (OpenAI's GPT-2 Large)\n",
    "- **Caching**: The model is stored in `hf_model_cache` for repeated use without reloading\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- The `pipeline()` function from Hugging Face's Transformers library provides a high-level API\n",
    "- Handles tokenization, model inference, and text decoding automatically\n",
    "- Downloads and caches the model on first use\n",
    "\n",
    "## Commented Alternative\n",
    "\n",
    "The commented line demonstrates an alternative approach:\n",
    "- Uses a locally downloaded version of the model\n",
    "- Extracts just the model name (without organization) using string splitting\n",
    "- Assumes the model exists in a local `models/` directory\n",
    "- Useful for offline use or when working with limited internet connectivity\n",
    "\n",
    "This pipeline simplifies working with powerful language models for creative text generation applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_cache = pipeline(\"text-generation\", model=hf_reference)\n",
    "# hf_model_local = pipeline(\"text-generation\", model=f\"models/{hf_reference.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fce28f",
   "metadata": {},
   "source": [
    "# Generating Story Continuations with GPT-2\n",
    "\n",
    "This code generates creative story continuations for a classic fairy tale opening using the pre-configured GPT-2 Large text generation pipeline.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **`set_seed(42)`**: Sets a random seed for reproducible text generation results\n",
    "  - The value 42 is commonly used as a default seed in programming\n",
    "  - Ensures the same text is generated each time the code runs\n",
    "\n",
    "- **Prompt**: \"once upon a time ...\"\n",
    "  - Classic fairy tale opening that primes the model for narrative generation\n",
    "  - The ellipsis invites the model to continue the thought\n",
    "\n",
    "- **Generation Parameters**:\n",
    "  - `max_length=50`: Limits output to 50 tokens (roughly 30-40 words)\n",
    "  - `num_return_sequences=2`: Generates two different story continuations\n",
    "  - `truncation=False`: Allows the output to reach the full max_length\n",
    "\n",
    "- **Output Processing**:\n",
    "  - Iterates through the generated sequences\n",
    "  - Prints each complete story continuation\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The code will produce two different story continuations that both begin with \"once upon a time ...\" but diverge based on the probabilistic sampling of the language model. With seed 42, these continuations will be consistent across runs.\n",
    "\n",
    "This demonstrates how a large language model can be used for creative writing assistance or story generation from simple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "text = \"once upon a time ...\"\n",
    "generated_text = hf_model_cache(text, max_length=50, num_return_sequences=2, truncation=False)\n",
    "\n",
    "for i in generated_text:\n",
    "  print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827796a5",
   "metadata": {},
   "source": [
    "\n",
    "# Serialize and Save Model Information from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code demonstrates how to retrieve, serialize, and save detailed model information from the Hugging Face Hub:\n",
    "\n",
    "1. **Serialization Function**: The `serialize_object()` function handles complex objects recursively:\n",
    "   - Converts datetime objects to ISO format strings\n",
    "   - Transforms objects with `__dict__` attributes into dictionaries\n",
    "   - Processes nested lists and dictionaries\n",
    "   - Preserves primitive data types\n",
    "\n",
    "2. **API Interaction**: Creates an instance of the Hugging Face API client\n",
    "\n",
    "3. **Model Information**: Fetches comprehensive metadata about the specified model using `api.model_info()`\n",
    "\n",
    "4. **File Operations**: \n",
    "   - Extracts the model name from the reference path\n",
    "   - Creates a JSON file named after the model\n",
    "   - Serializes the model information and writes it to the file\n",
    "\n",
    "This allows for local storage of model metadata for later reference or analysis, particularly useful for model governance, versioning, and documentation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5a8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "api = HfApi()\n",
    "with open(f\"models/{hf_reference.split('/')[1]}.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(serialize_object(api.model_info(hf_reference))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
