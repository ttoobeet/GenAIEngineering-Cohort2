{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536ac465",
   "metadata": {},
   "source": [
    "Google Colab: https://colab.research.google.com/drive/1eyH3H62rv5sXxoo3ytavpotvB6jbyYX1?usp=sharing\n",
    "\n",
    "HuggingFace: https://huggingface.co/facebook/bart-large-mnli\n",
    "\n",
    "## Libraries Explained\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file into the application's environment, helping manage configuration separately from code.\n",
    "\n",
    "- **huggingface_hub**: \n",
    "  - **HfApi**: Provides programmatic access to the Hugging Face model hub for uploading, downloading, and managing models.\n",
    "  - **hf_hub_download**: Simplifies downloading model files from the Hugging Face hub to your local environment.\n",
    "\n",
    "- **transformers**: Offers pre-trained models for natural language processing tasks. The `pipeline` function specifically provides an easy-to-use interface for common NLP tasks like text generation, sentiment analysis, and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7002eb",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff45713",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Model Reference\n",
    "\n",
    "[facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\n",
    "\n",
    "# Facebook BART Large MNLI Model\n",
    "\n",
    "## Model Overview\n",
    "This reference points to Facebook AI's BART large model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset.\n",
    "\n",
    "## Key Specifications\n",
    "- **Architecture**: BART (Bidirectional and Auto-Regressive Transformers)\n",
    "- **Size**: Large (400M parameters)\n",
    "- **Fine-tuning**: MNLI (Multi-Genre Natural Language Inference)\n",
    "- **Developer**: Facebook AI Research (Meta AI)\n",
    "- **Use Case**: Zero-shot classification and natural language inference\n",
    "\n",
    "## Capabilities\n",
    "- Determines textual entailment between premise and hypothesis\n",
    "- Classifies relationship as entailment, contradiction, or neutral\n",
    "- Enables zero-shot classification by framing categories as hypotheses\n",
    "- Generalizes to unseen tasks and domains without additional training\n",
    "\n",
    "\n",
    "\n",
    "This model serves as a powerful foundation for flexible text classification tasks where predefined categories or training examples may not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c361954",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='facebook/bart-large-mnli'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4673a",
   "metadata": {},
   "source": [
    "\n",
    "# Downloading Specific Model Files from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code snippet demonstrates how to selectively download specific files from a Hugging Face model repository:\n",
    "\n",
    "1. **File Definition**: First, a list of commonly required files for transformer models is defined, with comments explaining each file's purpose:\n",
    "   - Vocabulary files for tokenization\n",
    "   - Configuration files for model architecture\n",
    "   - Tokenizer files for text preprocessing\n",
    "   - Model weights in different formats (PyTorch and SafeTensors)\n",
    "\n",
    "2. **Selective Download**: The code iterates through each file in the list and:\n",
    "   - Attempts to download it using `hf_hub_download()`\n",
    "   - Specifies the model repository via `repo_id=hf_reference`\n",
    "   - Saves files to a local directory structure based on the model name\n",
    "   - Prints the local path where each file is saved\n",
    "\n",
    "3. **Error Handling**: The try-except block catches and reports any download failures, allowing the process to continue even if certain files aren't available for the specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of required files\n",
    "# required_files = [\n",
    "#     \"vocab.txt\",          # Vocabulary file (if applicable)\n",
    "#     \"vocab.json\",          # Vocabulary file (if applicable)       \n",
    "#     \"config.json\",        # Model configuration\n",
    "#     \"tokenizer.json\",     # Tokenizer configuration (if applicable)\n",
    "#     \"merges.txt\",         # BPE merge rules file (if applicable)\n",
    "#     \"pytorch_model.bin\",  # Model weights\n",
    "#     \"model.safetensors\",  # Alternative model weights format\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Download only the required files\n",
    "# for file_name in required_files:\n",
    "#     try:\n",
    "#         print()\n",
    "#         print(f\"Attempting to download: {file_name}\")\n",
    "#         local_path = hf_hub_download(repo_id=hf_reference, filename=file_name, local_dir=f\"models/{hf_reference.split('/')[1]}\")\n",
    "#         print(f\"Saved to: {local_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not download {file_name}: {e}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f50228",
   "metadata": {},
   "source": [
    "\n",
    "# Creating Zero-shot Classification Pipelines\n",
    "\n",
    "\n",
    "This code initializes two sentiment analysis pipelines using Hugging Face's `transformers` library:\n",
    "\n",
    "1. **Cached Model Pipeline**: \n",
    "   - `hf_model_cache` uses the model identifier directly (`hf_reference`)\n",
    "   - When this pipeline runs, it will first check the default Hugging Face cache directory on your system\n",
    "   - If not found in cache, it automatically downloads the model from Hugging Face Hub\n",
    "\n",
    "2. **Local Model Pipeline**:\n",
    "   - `hf_model_local` uses the previously downloaded model files\n",
    "   - Points to the local directory where model files were saved earlier\n",
    "   - Loads the model from the local files rather than downloading or using cache\n",
    "   - Path is constructed by extracting just the model name from the reference\n",
    "\n",
    "Both pipelines provide the same sentiment analysis functionality but differ in where they source the model files from, allowing flexibility between network-dependent and offline usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_cache = pipeline(\"zero-shot-classification\", model=hf_reference)\n",
    "# hf_model_local = pipeline(\"zero-shot-classification\", model=f\"models/{hf_reference.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fce28f",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification of Technology Text\n",
    "\n",
    "\n",
    "This code performs zero-shot classification on a technology-related statement using a cached Hugging Face model:\n",
    "\n",
    "### Input Components:\n",
    "- **Text**: A statement about organizations using \"ubiquitous computing fabric\" for mission-critical applications\n",
    "- **Candidate Labels**: Five potential categories for classification\n",
    "  - education\n",
    "  - politics\n",
    "  - technology\n",
    "  - science\n",
    "  - cosmology\n",
    "- **Model**: Pre-loaded/cached Hugging Face model (likely BART or similar transformer)\n",
    "\n",
    "### Process:\n",
    "1. The text describes cloud-to-edge computing infrastructure\n",
    "2. The model analyzes this text against each potential category\n",
    "3. No training examples are provided (zero-shot approach)\n",
    "4. The model determines how likely the text belongs to each category\n",
    "\n",
    "### Expected Output:\n",
    "The result variable will contain a dictionary with:\n",
    "- The original input sequence\n",
    "- Labels ranked by likelihood\n",
    "- Confidence scores for each label\n",
    "\n",
    "The model will likely classify this as primarily \"technology\" related given keywords like \"computing fabric,\" \"cloud,\" \"edge,\" and \"applications,\" with perhaps some association to \"science\" as a secondary category.\n",
    "\n",
    "This demonstrates how zero-shot classification can identify content categories without specific training examples for each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"organizations continue to choose our ubiquitous computing fabric—from cloud to edge—to run their missioncritical applications\"\n",
    "candidate_labels = [\"education\", \"politics\", \"technology\", \"science\", \"cosmology\"]\n",
    "result = hf_model_cache(text, candidate_labels=candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb652c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"which is the most habital planet in the future?\"\n",
    "candidate_labels = [\"education\", \"politics\", \"technology\", \"science\", \"cosmology\"]\n",
    "result = hf_model_cache(text, candidate_labels=candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827796a5",
   "metadata": {},
   "source": [
    "\n",
    "# Serialize and Save Model Information from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code demonstrates how to retrieve, serialize, and save detailed model information from the Hugging Face Hub:\n",
    "\n",
    "1. **Serialization Function**: The `serialize_object()` function handles complex objects recursively:\n",
    "   - Converts datetime objects to ISO format strings\n",
    "   - Transforms objects with `__dict__` attributes into dictionaries\n",
    "   - Processes nested lists and dictionaries\n",
    "   - Preserves primitive data types\n",
    "\n",
    "2. **API Interaction**: Creates an instance of the Hugging Face API client\n",
    "\n",
    "3. **Model Information**: Fetches comprehensive metadata about the specified model using `api.model_info()`\n",
    "\n",
    "4. **File Operations**: \n",
    "   - Extracts the model name from the reference path\n",
    "   - Creates a JSON file named after the model\n",
    "   - Serializes the model information and writes it to the file\n",
    "\n",
    "This allows for local storage of model metadata for later reference or analysis, particularly useful for model governance, versioning, and documentation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "api = HfApi()\n",
    "with open(f\"models/{hf_reference.split('/')[1]}.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(serialize_object(api.model_info(hf_reference))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
