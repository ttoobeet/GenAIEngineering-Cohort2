{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536ac465",
   "metadata": {},
   "source": [
    "Google Colab: https://colab.research.google.com/drive/1o0JFJAF831rQW6FYIG3-3t3w-xmJqpGf?usp=sharing\n",
    "\n",
    "HuggingFace: https://huggingface.co/deepset/roberta-base-squad2\n",
    "\n",
    "## Libraries Explained\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file into the application's environment, helping manage configuration separately from code.\n",
    "\n",
    "- **huggingface_hub**: \n",
    "  - **HfApi**: Provides programmatic access to the Hugging Face model hub for uploading, downloading, and managing models.\n",
    "  - **hf_hub_download**: Simplifies downloading model files from the Hugging Face hub to your local environment.\n",
    "\n",
    "- **transformers**: Offers pre-trained models for natural language processing tasks. The `pipeline` function specifically provides an easy-to-use interface for common NLP tasks like text generation, sentiment analysis, and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7002eb",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff45713",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Model Reference\n",
    "\n",
    "[deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)\n",
    "\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "This reference points to a RoBERTa model fine-tuned specifically for question answering tasks using the SQuAD 2.0 dataset.\n",
    "\n",
    "## Key Specifications\n",
    "\n",
    "- **Base Architecture**: RoBERTa (Robustly Optimized BERT Approach)\n",
    "- **Model Size**: Base variant (125M parameters)\n",
    "- **Developer**: deepset AI\n",
    "- **Fine-tuning**: SQuAD 2.0 (Stanford Question Answering Dataset)\n",
    "- **Task**: Extractive question answering\n",
    "\n",
    "## Model Capabilities\n",
    "\n",
    "- Answers questions by locating relevant spans within provided context\n",
    "- Handles unanswerable questions (a key feature of SQuAD 2.0)\n",
    "- Extracts precise answers rather than generating them\n",
    "- Works best with factual questions about information present in text\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "- Input: Question and context passages\n",
    "- Output: Start and end positions of answer span within context\n",
    "- Optimization: Trained on both answerable and unanswerable questions\n",
    "- Language: English\n",
    "\n",
    "\n",
    "This model is particularly useful for applications requiring information extraction from documents, search engines, and question-answering systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c361954",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='deepset/roberta-base-squad2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4673a",
   "metadata": {},
   "source": [
    "\n",
    "# Downloading Specific Model Files from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code snippet demonstrates how to selectively download specific files from a Hugging Face model repository:\n",
    "\n",
    "1. **File Definition**: First, a list of commonly required files for transformer models is defined, with comments explaining each file's purpose:\n",
    "   - Vocabulary files for tokenization\n",
    "   - Configuration files for model architecture\n",
    "   - Tokenizer files for text preprocessing\n",
    "   - Model weights in different formats (PyTorch and SafeTensors)\n",
    "\n",
    "2. **Selective Download**: The code iterates through each file in the list and:\n",
    "   - Attempts to download it using `hf_hub_download()`\n",
    "   - Specifies the model repository via `repo_id=hf_reference`\n",
    "   - Saves files to a local directory structure based on the model name\n",
    "   - Prints the local path where each file is saved\n",
    "\n",
    "3. **Error Handling**: The try-except block catches and reports any download failures, allowing the process to continue even if certain files aren't available for the specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of required files\n",
    "required_files = [\n",
    "    \"vocab.txt\",          # Vocabulary file (if applicable)\n",
    "    \"vocab.json\",          # Vocabulary file (if applicable)       \n",
    "    \"config.json\",        # Model configuration\n",
    "    \"tokenizer.json\",     # Tokenizer configuration (if applicable)\n",
    "    \"merges.txt\",         # BPE merge rules file (if applicable)\n",
    "    \"pytorch_model.bin\",  # Model weights\n",
    "    \"model.safetensors\",  # Alternative model weights format\n",
    "]\n",
    "\n",
    "\n",
    "# Download only the required files\n",
    "for file_name in required_files:\n",
    "    try:\n",
    "        print()\n",
    "        print(f\"Attempting to download: {file_name}\")\n",
    "        local_path = hf_hub_download(repo_id=hf_reference, filename=file_name, local_dir=f\"models/{hf_reference.split('/')[1]}\")\n",
    "        print(f\"Saved to: {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {file_name}: {e}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f50228",
   "metadata": {},
   "source": [
    "# Setting Up Question Answering Pipelines\n",
    "\n",
    "This code initializes two identical question answering pipelines using different model sources:\n",
    "\n",
    "### Remote Model (Cached)\n",
    "- `hf_model_cache`: Uses the model directly from Hugging Face's model hub\n",
    "  - References `hf_reference` (\"deepset/roberta-base-squad2\")\n",
    "  - Downloads and caches the model on first use\n",
    "  - Automatically fetches the latest version\n",
    "  - Requires internet connection initially\n",
    "\n",
    "### Local Model\n",
    "- `hf_model_local`: Uses a locally saved version of the same model\n",
    "  - Path: `\"models/roberta-base-squad2\"`\n",
    "  - Extracts model name from the reference using string splitting\n",
    "  - Assumes the model has been previously downloaded to the local directory\n",
    "  - Works offline after download\n",
    "\n",
    "## Pipeline Functionality\n",
    "\n",
    "Both pipelines provide:\n",
    "- Extractive question answering capabilities\n",
    "- Answer span identification in provided context\n",
    "- Confidence scores for predicted answers\n",
    "- Handling of unanswerable questions\n",
    "\n",
    "## Usage Considerations\n",
    "\n",
    "- **Remote/Cached**: Best for initial development or when storage is limited\n",
    "- **Local**: Preferred for production, offline usage, or repeated access\n",
    "- Both versions offer identical functionality and performance\n",
    "- The local version avoids repeated downloads and potential API rate limits\n",
    "\n",
    "This dual setup provides flexibility while ensuring consistent behavior across different deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ff4823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "hf_model_cache = pipeline(\"question-answering\", model=hf_reference)\n",
    "# hf_model_local = pipeline(\"question-answering\", model=f\"models/{hf_reference.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fce28f",
   "metadata": {},
   "source": [
    "# Extracting Customer Responsibilities with Question Answering\n",
    "\n",
    "This example demonstrates extractive question answering on a legal disclaimer text:\n",
    "\n",
    "### Input Components\n",
    "- **Query**: \"what are customer's responsibilities\"\n",
    "  - Seeks to identify obligations or responsibilities mentioned in the text\n",
    "  - Phrased as a straightforward question\n",
    "\n",
    "- **Context**: NVIDIA legal disclaimer text\n",
    "  - Contains information about limitations of liability\n",
    "  - Defines what NVIDIA is and isn't responsible for\n",
    "  - Typical boilerplate legal language\n",
    "\n",
    "### Question Answering Process\n",
    "- **Model**: deepset/roberta-base-squad2 (referenced through hf_model_cache)\n",
    "- **Parameters**:\n",
    "  - `top_k=3`: Returns the 3 most likely answers instead of just the best one\n",
    "  - This provides alternative interpretations of what might constitute \"responsibilities\"\n",
    "\n",
    "### Output Format\n",
    "The code will print three different potential answers, each containing:\n",
    "- The extracted answer text span\n",
    "- Start and end positions within the context\n",
    "- Confidence score for each answer\n",
    "- Separator lines between results\n",
    "\n",
    "Since the text doesn't explicitly list customer responsibilities but rather NVIDIA's non-responsibilities, the model will likely extract phrases related to liability, use of information, or patent infringement as possible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbf2fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0008512335480190814, 'start': 371, 'end': 394, 'answer': 'errors contained herein'}\n",
      "*****************\n",
      "{'score': 0.0005820643855258822, 'start': 367, 'end': 394, 'answer': 'any errors contained herein'}\n",
      "*****************\n",
      "{'score': 0.0005411853780969977, 'start': 345, 'end': 394, 'answer': 'no responsibility for any errors contained herein'}\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "query = \"what are customer's responsibilities\"\n",
    "text = '''This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA\n",
    "Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and\n",
    "assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents\n",
    "or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or\n",
    "functionality.'''\n",
    "\n",
    "# Get answers from different models\n",
    "results = hf_model_cache(question=query, context=text, top_k=3)\n",
    "\n",
    "for result in results:\n",
    "  print(result)\n",
    "  print(\"*****************\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26689c14",
   "metadata": {},
   "source": [
    "\n",
    "# Serialize and Save Model Information from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code demonstrates how to retrieve, serialize, and save detailed model information from the Hugging Face Hub:\n",
    "\n",
    "1. **Serialization Function**: The `serialize_object()` function handles complex objects recursively:\n",
    "   - Converts datetime objects to ISO format strings\n",
    "   - Transforms objects with `__dict__` attributes into dictionaries\n",
    "   - Processes nested lists and dictionaries\n",
    "   - Preserves primitive data types\n",
    "\n",
    "2. **API Interaction**: Creates an instance of the Hugging Face API client\n",
    "\n",
    "3. **Model Information**: Fetches comprehensive metadata about the specified model using `api.model_info()`\n",
    "\n",
    "4. **File Operations**: \n",
    "   - Extracts the model name from the reference path\n",
    "   - Creates a JSON file named after the model\n",
    "   - Serializes the model information and writes it to the file\n",
    "\n",
    "This allows for local storage of model metadata for later reference or analysis, particularly useful for model governance, versioning, and documentation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "api = HfApi()\n",
    "with open(f\"models/{hf_reference.split('/')[1]}.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(serialize_object(api.model_info(hf_reference))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
