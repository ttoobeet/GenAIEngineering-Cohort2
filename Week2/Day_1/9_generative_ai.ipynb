{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536ac465",
   "metadata": {},
   "source": [
    "Google Colab: https://colab.research.google.com/drive/1OpWlDGd6HnD4bmzEtywARctu2riKfFJk?usp=sharing\n",
    "\n",
    "HuggingFace: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "\n",
    "## Libraries Explained\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file into the application's environment, helping manage configuration separately from code.\n",
    "\n",
    "- **huggingface_hub**: \n",
    "  - **HfApi**: Provides programmatic access to the Hugging Face model hub for uploading, downloading, and managing models.\n",
    "  - **hf_hub_download**: Simplifies downloading model files from the Hugging Face hub to your local environment.\n",
    "\n",
    "- **transformers**: Offers pre-trained models for natural language processing tasks. The `pipeline` function specifically provides an easy-to-use interface for common NLP tasks like text generation, sentiment analysis, and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7002eb",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff45713",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Model Reference\n",
    "\n",
    "[mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "\n",
    "# Mistral 7B Instruct Model\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "This reference points to Mistral AI's 7B-parameter instruction-tuned language model, version 0.3, hosted on the Hugging Face model hub.\n",
    "\n",
    "## Key Specifications\n",
    "\n",
    "- **Architecture**: Transformer-based language model\n",
    "- **Size**: 7 billion parameters (compact but powerful)\n",
    "- **Type**: Instruction-tuned generative model\n",
    "- **Version**: 0.3 (latest stable release)\n",
    "- **Developer**: Mistral AI\n",
    "- **License**: Apache 2.0\n",
    "\n",
    "## Model Capabilities\n",
    "\n",
    "- Follows natural language instructions\n",
    "- Generates coherent and contextually relevant text\n",
    "- Handles various tasks through prompting:\n",
    "  - Question answering\n",
    "  - Summarization\n",
    "  - Creative writing\n",
    "  - Explanations\n",
    "  - Reasoning\n",
    "  - Code generation\n",
    "\n",
    "## Technical Features\n",
    "\n",
    "- Efficient architecture with strong performance for its size\n",
    "- Sliding Window Attention mechanism for handling longer contexts\n",
    "- Grouped-query attention for improved inference speed\n",
    "- Suitable for deployment on consumer-grade hardware\n",
    "\n",
    "This model represents a good balance between capability and resource requirements, making it accessible for various applications while delivering performance competitive with much larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c361954",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='mistralai/Mistral-7B-Instruct-v0.3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc91457",
   "metadata": {},
   "source": [
    "# Making API Calls to Mistral 7B Instruct\n",
    "\n",
    "\n",
    "## Function Explanation\n",
    "\n",
    "This code demonstrates how to interact with the Mistral 7B Instruct model through Hugging Face's Inference API:\n",
    "\n",
    "### API Configuration\n",
    "- **Endpoint**: Uses Hugging Face's router API with Together AI as the provider\n",
    "- **Authentication**: Bearer token authorization with your Hugging Face API key\n",
    "- **Model**: Specifies Mistral 7B Instruct v0.3\n",
    "\n",
    "### Query Function\n",
    "- **Purpose**: Encapsulates the API call logic for reusability\n",
    "- **Input**: Takes a payload dictionary with messages and parameters\n",
    "- **Process**: Sends a POST request to the API endpoint\n",
    "- **Output**: Returns the JSON response from the API\n",
    "\n",
    "### Request Structure\n",
    "- **Messages Array**: Contains conversation history in a chat format\n",
    "  - Each message has a \"role\" (user/assistant) and \"content\"\n",
    "  - This example has a single user message asking about France's capital\n",
    "- **Max Tokens**: Limits the response length to 512 tokens\n",
    "- **Model**: Explicitly specifies which model to use\n",
    "\n",
    "### Response Handling\n",
    "- Extracts the text response from the nested JSON structure\n",
    "- `response[\"choices\"][0][\"message\"][\"content\"]` accesses the actual text reply\n",
    "\n",
    "This approach follows the standard chat completions API format used by many modern LLMs, making it adaptable to other models with minimal changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b8a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's a simple example of recursion in Python using the Fibonacci sequence. This code defines a function `fibonacci(n)` that calculates the nth number in the Fibonacci sequence.\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Test the function\n",
      "print(fibonacci(10))  # Output: 55\n",
      "```\n",
      "\n",
      "In this example, the base case is when `n` is less than or equal to 1, where the function returns `n`. For other values of `n`, the function calls itself recursively with `n-1` and `n-2` as arguments until it reaches the base case.\n",
      "\n",
      "When you run this code, it will calculate the 10th Fibonacci number (55) by repeatedly calling the `fibonacci` function. Each call to the function will eventually reach the base case and return a value, which will be added to the return value of the previous call until the final result is calculated. This demonstrates the concept of recursion in Python.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/together/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {hf_key}\",\n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "response = query({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"write a code to explain recursion in python?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "})\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26689c14",
   "metadata": {},
   "source": [
    "\n",
    "# Serialize and Save Model Information from Hugging Face Hub\n",
    "\n",
    "\n",
    "This code demonstrates how to retrieve, serialize, and save detailed model information from the Hugging Face Hub:\n",
    "\n",
    "1. **Serialization Function**: The `serialize_object()` function handles complex objects recursively:\n",
    "   - Converts datetime objects to ISO format strings\n",
    "   - Transforms objects with `__dict__` attributes into dictionaries\n",
    "   - Processes nested lists and dictionaries\n",
    "   - Preserves primitive data types\n",
    "\n",
    "2. **API Interaction**: Creates an instance of the Hugging Face API client\n",
    "\n",
    "3. **Model Information**: Fetches comprehensive metadata about the specified model using `api.model_info()`\n",
    "\n",
    "4. **File Operations**: \n",
    "   - Extracts the model name from the reference path\n",
    "   - Creates a JSON file named after the model\n",
    "   - Serializes the model information and writes it to the file\n",
    "\n",
    "This allows for local storage of model metadata for later reference or analysis, particularly useful for model governance, versioning, and documentation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5a8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "api = HfApi()\n",
    "with open(f\"models/{hf_reference.split('/')[1]}.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(serialize_object(api.model_info(hf_reference))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
